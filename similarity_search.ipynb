{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import re\n",
    "from langchain.embeddings import HuggingFaceHubEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Chapter 14. Data Warehousing,OLAP, and Data MiningCHAPTER 14DATA WAREHOUSING, OLAP, AND DATA MININGAfter reading this chapter, the reader will understand:What are OLTP systems?The need of a data warehouseBasic characteristics of a data warehouse, which include subjectoriented, non-volatile, time varying, and integrated characteristicsDifferences between data warehouse and OLTP systemsThe entire process of getting data into the data warehouse, which istermed as extract, transform, and load (ETL) processThe importance of metadata repositoryThe multidimensional data model for a data warehouseThe concept of fact and dimension tablesThe two ways of representing multidimensional data in a datawarehouse, which include star schema and snowflake schemaThe online analytical processing (OLAP) technology, that enablesanalysts, managers, and executives to analyse the complex dataderived from the data warehouseVarious functionalities provided by OLAP system, which includepivoting, slicing and dicing, and rollup and drill downThe three ways of implementing OLAPThe data mining technologyThe process of knowledge discovery in database (KDD)Different types of information discovered during data mining, whichinclude association rules, classification, and clusteringApplication areas of data mining technologyOver the past few decades, organizations have built large databases by', metadata={'source': 'Data Warehousing, OLAP, and Data Mining.pdf', 'page': 0})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read file\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"Data Warehousing, OLAP, and Data Mining.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make chunks\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(        \n",
    "    separator = ' ',\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")\n",
    "docs = text_splitter.split_documents(pages)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='from multiple sources, stored under a unified schema, at a single site. Itcan successfully answer any ad hoc, complex, statistical or analyticalqueries. The data once gathered can be stored for a longer periodallowing access to historical data. The data warehouses provide the usera single consolidated interface to data, which makes decision-supportqueries easier to write.Learn MoreA data mart is a subset of data warehouse that is designed for aparticular department of an organization such as sales, marketing, orfinance.NOTE A query that is run at the spur of the moment, and generally isnever saved to run again is known as ad hoc query. These queries aregenerally not predefined, and are built and run as and when required.Since data warehouses have been developed in numerous organizationsto meet their specific needs, there is no single and standard definition ofthis term. According to William Inmon, the father of the modern datawarehouse, a data warehouse is a subject-oriented,' metadata={'source': 'Data Warehousing, OLAP, and Data Mining.pdf', 'page': 2}\n"
     ]
    }
   ],
   "source": [
    "print(docs[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harsh\\Desktop\\NEU\\LLM Project\\.venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "You're using a different task than the one specified in the repository. Be sure to know what you're doing :)\n"
     ]
    }
   ],
   "source": [
    "# Embed text and store in vector space\n",
    "from langchain.embeddings import HuggingFaceHubEmbeddings\n",
    "load_dotenv()\n",
    "# repo_id = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "repo_id = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "hf = HuggingFaceHubEmbeddings(\n",
    "    repo_id= repo_id\n",
    ")\n",
    "\n",
    "# load it into Chroma db\n",
    "from langchain.vectorstores import Chroma\n",
    "db = Chroma.from_documents(docs, hf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query on similarity search\n",
    "query = \"What is a Data Warehouse?\"\n",
    "result = db.similarity_search(query, k = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warehouse, a data warehouse is a subject-oriented, integrated,time-variant, non-volatile collection of data in support ofmanagement’s decisions.6. The main advantage of using a data warehouse is that a data analystcan perform complex queries and analyses on the information storedin data warehouse without affecting the OLTP systems.7. A data warehouse basically consists of three components, namely,the data sources, the ETL, and the schema of data stored in datawarehouse including the metadata.8. While creating a data warehouse, the data is extracted from multiple,heterogeneous data sources. Then data cleansing is performed toremove any kind of minor errors and inconsistencies. This data isthen transformed to accommodate semantic mismatches. Thecleaned and transformed data is finally loaded into the warehouse.9. The entire process of getting data into the data warehouse is calledextract, transform, and load (ETL) process.10. The third and the most important component of the data\n"
     ]
    }
   ],
   "source": [
    "output = result[0].page_content\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warehouse, a data warehouse is a subject-oriented, integrated,time-variant, non-volatile collection of data in support ofmanagement’s decisions.6. The main advantage of using a data warehouse is that a data analystcan perform complex queries and analyses on the information storedin data warehouse without affecting the OLTP systems.7. A data warehouse basically consists of three components, namely,the data sources, the ETL, and the schema of data stored in datawarehouse including the metadata.8. While creating a data warehouse, the data is extracted from multiple,heterogeneous data sources. Then data cleansing is performed toremove any kind of minor errors and inconsistencies. This data isthen transformed to accommodate semantic mismatches. Thecleaned and transformed data is finally loaded into the warehouse.9. The entire process of getting data into the data warehouse is calledextract, transform, and load (ETL) process.10. The third and the most important component of the data\n"
     ]
    }
   ],
   "source": [
    "# Minimum scores is the best score\n",
    "similarity_scores = {i:d[1] for i,d in enumerate(db.similarity_search_with_score(query, k=5 ))}\n",
    "\n",
    "ss = pd.Series(similarity_scores)\n",
    "\n",
    "best_idx = ss.idxmin()\n",
    "\n",
    "output_based_on_scores = result[best_idx].page_content\n",
    "print(output_based_on_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"\"\n",
    "# pdf_text = PdfReader(\"Data Warehousing, OLAP, and Data Mining.pdf\")\n",
    "# for page_num in range(len(pdf_text.pages)):\n",
    "#     page_text = pdf_text.pages[page_num]\n",
    "#     text += page_text.extract_text()\n",
    "\n",
    "\n",
    "# text = re.sub(' +', ' ', text)\n",
    "# print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = CharacterTextSplitter(        \n",
    "#     separator = \"\\n\",\n",
    "#     chunk_size = 1000,\n",
    "#     chunk_overlap  = 200,\n",
    "#     length_function = len,\n",
    "#     is_separator_regex = False,\n",
    "# )\n",
    "\n",
    "# chunks_text = text_splitter.split_text(text)\n",
    "# chunks_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repo_id = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "# hf = HuggingFaceHubEmbeddings(\n",
    "#     repo_id= repo_id\n",
    "# )\n",
    "\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "# embeddings = model.encode(chunks_text)\n",
    "# print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it into Chroma db\n",
    "# from langchain.vectorstores import Chroma\n",
    "# db = Chroma.from_texts(text, hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chroma.from_texts(text, hf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
